{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRlMECj9Aud3"
      },
      "source": [
        "# Pipeline de Processamento de Linguagem Natural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV-8IWpJcR32"
      },
      "source": [
        "Passos que foram considerados para a realização da pipeline\n",
        "---\n",
        "Segmentação\n",
        "\n",
        "Tokenização\n",
        "\n",
        "Lematização\n",
        "\n",
        "Stemming\n",
        "\n",
        "Remoção de Stop Words\n",
        "\n",
        "Rotulação de Part Of Speech\n",
        "\n",
        "Reconhecimento de Entidades Nomeadas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXhMa49hA8LR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g4stBeUA8LR"
      },
      "source": [
        "## Preparação de ambiente para o algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EvkzZ3NdFrj",
        "outputId": "b66860dc-6cd2-4eb2-d2c7-73597ab46017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgB6ZUiddKVJ"
      },
      "outputs": [],
      "source": [
        "# Importação de lib\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwFOJBx7dJHr",
        "outputId": "004e91b9-6080-4d22-a6be-ec2d713f8be4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')  # Baixa os dados necessários\n",
        "nltk.download('wordnet')  # Baixa os dados para lematização\n",
        "nltk.download('averaged_perceptron_tagger')  # Baixa os dados para rotulação de Part of Speech\n",
        "nltk.download('maxent_ne_chunker')  # Baixa os dados para reconhecimento de Entidades Nomeadas\n",
        "nltk.download('words')  # Baixa os dados adicionais\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLrk3zZ3A8LT"
      },
      "source": [
        "## Função universal - todos os passos do processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_cJf_z2bdYMz"
      },
      "outputs": [],
      "source": [
        "def process_nlp(texto):\n",
        "\n",
        "    # Segmentação em frases\n",
        "    frases = sent_tokenize(texto)\n",
        "\n",
        "    # Tokenização e segmentação em palavras\n",
        "    tokens_por_frase = [word_tokenize(frase) for frase in frases]\n",
        "\n",
        "    # Inicializa o stemmer e lematizador\n",
        "    stemmer = PorterStemmer()\n",
        "    lematizador = WordNetLemmatizer()\n",
        "\n",
        "    # Processamento de cada token\n",
        "    tokens_processados = []\n",
        "\n",
        "    for tokens_frase in tokens_por_frase:\n",
        "        tokens_filtrados = [token.lower() for token in tokens_frase if token.isalpha()]\n",
        "\n",
        "        # Stemming e lematização\n",
        "        tokens_stemmed = [stemmer.stem(token) for token in tokens_filtrados]\n",
        "        tokens_lematizados = [lematizador.lemmatize(token) for token in tokens_stemmed]\n",
        "\n",
        "        # Remoção de stop words\n",
        "        stopwords_en = set(stopwords.words('portuguese'))\n",
        "        tokens_sem_stopwords = [token for token in tokens_lematizados if token not in stopwords_en]\n",
        "\n",
        "        tokens_processados.append(tokens_sem_stopwords)\n",
        "\n",
        "    # Rotulação de Part of Speech\n",
        "    pos_tags = [pos_tag(tokens) for tokens in tokens_processados]\n",
        "\n",
        "    # Reconhecimento de Entidades Nomeadas\n",
        "    entidades_nomeadas = [ne_chunk(tagged_tokens) for tagged_tokens in pos_tags]\n",
        "\n",
        "    return frases, tokens_processados, pos_tags, entidades_nomeadas\n",
        "\n",
        "texto_exemplo = \"A corrida aconteceu nos campos enquanto os cavalos corriam. João estava assistindo.\"\n",
        "frases, tokens, pos_tags, entidades = process_nlp(texto_exemplo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpbD_bNReKDk",
        "outputId": "0c9509fc-d85a-4dc9-d353-6ea0bbf2ed08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('corrida', 'NN'),\n",
              "  ('aconteceu', 'NN'),\n",
              "  ('campo', 'NN'),\n",
              "  ('enquanto', 'IN'),\n",
              "  ('cavalo', 'NN'),\n",
              "  ('corriam', 'NN')],\n",
              " [('joão', 'NN'), ('assistindo', 'NN')]]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BizYVZd7orw0"
      },
      "source": [
        "# Definição de funções unitárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VhdVN1jik6wv"
      },
      "outputs": [],
      "source": [
        "#Tokenização e segmentação em frases\n",
        "def sentence_token(text):\n",
        "  sentence = sent_tokenize(text)\n",
        "  return sentence\n",
        "\n",
        "\n",
        "# Tokenização e segmentação em palavras\n",
        "def tokenize(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        w = word_extraction(sentence)\n",
        "        words.extend(w)\n",
        "    words = sorted(list(set(words)))\n",
        "    return words\n",
        "\n",
        "# Remoção de stopwords\n",
        "def word_extraction(sentence):\n",
        "    ignore = set(stopwords.words('portuguese'))\n",
        "    words = re.sub(\"[^\\w]\", \" \", sentence).split()\n",
        "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
        "    return cleaned_text\n",
        "\n",
        "# Rotulação de Part of Speech\n",
        "def pos_tags(tokens_process):\n",
        "  #pos_tags = pos_tag(tokens_process)\n",
        "  pos_tags = pos_tag(tokens_process)\n",
        "  return pos_tags\n",
        "\n",
        "# Reconhecimento de Entidades Nomeadas\n",
        "def named_entities(pos_tags):\n",
        "   named_entities = [ne_chunk(tagged_tokens) for tagged_tokens in pos_tags]\n",
        "   return named_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yct1eoWN8Vgk"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yxixho_k8SNU"
      },
      "outputs": [],
      "source": [
        "def pipeline(text):\n",
        "  sentences = sentence_token(text)\n",
        "  word = tokenize(sentence)\n",
        "  classify = pos_tag(word)\n",
        "  return classify\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4WUSmdgkec7"
      },
      "source": [
        "## Testes unitários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH5vzThI79qY",
        "outputId": "9541e879-e840-41ac-b323-b3afa77386c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['O rato roeu a roupa do rato.', 'Ferrari paz amor.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Tokenização e segmentação em frases\n",
        "# Input: \"O rato roeu a roupa do rei de roma. Ferrari paz amor.\"\n",
        "# Output esperado: ['O rato roeu a roupa do rei de roma.', 'Ferrari paz amor.']\n",
        "\n",
        "\n",
        "texto = \"O rato roeu a roupa do rato. Ferrari paz amor.\"\n",
        "sentence = sentence_token(texto)\n",
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmA8PgTBb2ff",
        "outputId": "98de511e-c8b1-48ff-b1dc-609b2c88c86e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['amor', 'ferrari', 'o', 'paz', 'rato', 'roeu', 'roupa']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenização e segmentação em palavras\n",
        "# Input: ['O rato roeu a roupa do rei de roma.', 'Ferrari paz amor.']\n",
        "# Output esperado: ['amor', 'ferrari', 'o', 'paz', 'rato', 'roeu', 'roupa']\n",
        "\n",
        "texto = sentence\n",
        "word = tokenize(texto)\n",
        "word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkRJSmvSKyvj",
        "outputId": "627701d8-64b8-4312-c69f-24d6f4cbaef2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['o', 'rato', 'roeu', 'roupa', 'rato']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Processamento dos tokens\n",
        "# Input:['O rato roeu a roupa do rato.']\n",
        "# Output esperado: ['o', 'rato', 'roeu', 'roupa', 'rato']\n",
        "\n",
        "texto = sentence[0]\n",
        "process = word_extraction(texto)\n",
        "process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx28qX5UK-Bk",
        "outputId": "4334dce7-1663-42ef-8e32-3e643707fadb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('amor', 'NN'),\n",
              " ('ferrari', 'NN'),\n",
              " ('o', 'IN'),\n",
              " ('paz', 'NN'),\n",
              " ('rato', 'NN'),\n",
              " ('roeu', 'NN'),\n",
              " ('roupa', 'NN')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Rotulação de Part of Speech\n",
        "# Input: ['amor', 'ferrari', 'o', 'paz', 'rato', 'roeu', 'roupa']\n",
        "# Output esperado: [('amor', 'NN'), ('ferrari', 'NN'), ('o', 'NN'), ('paz', 'NN'), ('rato', 'NN'), ('roeu', 'NN'), ('roupa', 'NN')]\n",
        "\n",
        "texto = word\n",
        "pos_tags = pos_tags(texto)\n",
        "pos_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_kh2jwjkg7J"
      },
      "source": [
        "## Teste de Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTH45aGWFRJO",
        "outputId": "86552fd5-1624-4585-e0ef-09da31478122"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('amor', 'NN'),\n",
              " ('ferrari', 'NN'),\n",
              " ('o', 'IN'),\n",
              " ('paz', 'NN'),\n",
              " ('rato', 'NN'),\n",
              " ('roeu', 'NN'),\n",
              " ('roupa', 'NN')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texto = \"O rato roeu a roupa do rato\"\n",
        "pipeline(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m6lxF_QjCQk",
        "outputId": "bcf76054-93d6-4095-8942-1a29b8e85678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Frequência de Termos:\n",
            "[[1 1 1 6 1 1 1 1]\n",
            " [1 1 1 6 1 1 1 1]\n",
            " [1 1 1 6 1 1 1 1]\n",
            " [1 1 1 6 1 1 1 1]]\n",
            "\n",
            "Nomes das Palavras:\n",
            "['amor' 'ferrari' 'in' 'nn' 'paz' 'rato' 'roeu' 'roupa']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Documentos processados a partir da pipeline\n",
        "documentos = [\n",
        "    \"O rato roeu a roupa do rato.\",\n",
        "    \"Ferrari paz amor.\",\n",
        "    \"Outro documento de exemplo.\",\n",
        "    \"Algo mais para processar.\"\n",
        "]\n",
        "\n",
        "\n",
        "# Importe o CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Crie um objeto CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Processamento dos documentos e criação da matriz TF\n",
        "documentos_processados = []\n",
        "\n",
        "for doc in documentos:\n",
        "    tokens_processados = pipeline(doc)  # Use sua função pipeline\n",
        "    texto_processado = ' '.join([' '.join(tokens) for tokens in tokens_processados])\n",
        "    documentos_processados.append(texto_processado)\n",
        "\n",
        "matriz_tf = vectorizer.fit_transform(documentos_processados)\n",
        "\n",
        "# A matriz TF é uma matriz esparsa, podemos convertê-la para um array NumPy\n",
        "matriz_tf_array = matriz_tf.toarray()\n",
        "\n",
        "# Obtenha os nomes das palavras (features) como colunas da matriz\n",
        "nomes_palavras = vectorizer.get_feature_names_out()\n",
        "\n",
        "# A matriz   contém as contagens de termos\n",
        "print(\"Matriz de Frequência de Termos:\")\n",
        "print(matriz_tf_array)\n",
        "\n",
        "# E os nomes das palavras correspondentes\n",
        "print(\"\\nNomes das Palavras:\")\n",
        "print(nomes_palavras)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z799e7NWL39",
        "outputId": "07878d3a-ce1d-4520-8231-ff9832106c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Frequência de Termos:\n",
            "[[0 0 0 0 0 0 0 0 1 1 1]\n",
            " [0 1 0 0 1 0 1 0 0 0 0]\n",
            " [0 0 1 1 0 1 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 1 0 0 0]]\n",
            "\n",
            "Nomes das Palavras:\n",
            "['algo' 'amor' 'documento' 'exemplo' 'ferrari' 'outro' 'paz' 'processar'\n",
            " 'rato' 'roeu' 'roupa']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Documentos processados a partir da pipeline\n",
        "documentos = [\n",
        "    \"O rato roeu a roupa do rato.\",\n",
        "    \"Ferrari paz amor.\",\n",
        "    \"Outro documento de exemplo.\",\n",
        "    \"Algo mais para processar.\"\n",
        "]\n",
        "\n",
        "# Funções auxiliares\n",
        "def sentence_token(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "def tokenize(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        w = word_extraction(sentence)\n",
        "        words.extend(w)\n",
        "    words = sorted(list(set(words)))\n",
        "    return words\n",
        "\n",
        "def word_extraction(sentence):\n",
        "    ignore = set(stopwords.words('portuguese'))\n",
        "    words = re.sub(\"[^\\w]\", \" \", sentence).split()\n",
        "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
        "    return cleaned_text\n",
        "\n",
        "def pipeline(text):\n",
        "    sentences = sentence_token(text)\n",
        "    words = tokenize(sentences)\n",
        "    return words\n",
        "\n",
        "# Função principal\n",
        "def process_documents(documentos):\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    # Processamento dos documentos e criação da matriz TF\n",
        "    documentos_processados = []\n",
        "\n",
        "    for doc in documentos:\n",
        "        tokens_processados = pipeline(doc)  # Use sua função pipeline\n",
        "        texto_processado = ' '.join(tokens_processados)\n",
        "        documentos_processados.append(texto_processado)\n",
        "\n",
        "    matriz_tf = vectorizer.fit_transform(documentos_processados)\n",
        "\n",
        "    # A matriz TF é uma matriz esparsa, podemos convertê-la para um array NumPy\n",
        "    matriz_tf_array = matriz_tf.toarray()\n",
        "\n",
        "    # Obtenha os nomes das palavras (features) como colunas da matriz\n",
        "    nomes_palavras = vectorizer.get_feature_names_out()\n",
        "\n",
        "    return matriz_tf_array, nomes_palavras\n",
        "\n",
        "matriz_tf_array, nomes_palavras = process_documents(documentos)\n",
        "\n",
        "# A matriz contém as contagens de termos\n",
        "print(\"Matriz de Frequência de Termos:\")\n",
        "print(matriz_tf_array)\n",
        "\n",
        "# E os nomes das palavras correspondentes\n",
        "print(\"\\nNomes das Palavras:\")\n",
        "print(nomes_palavras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwxvbJr6jlFX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbo5H3UYs8Be",
        "outputId": "3bc63086-a559-41e4-fd03-cd2e86fdf34d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15963 sha256=c982d44986f24ed3b06c336a19db6094821e297387c2f9979d7c449d08854151\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7c/4c/632fba2ea8e88d8890102eb07bc922e1ca8fa14db5902c91a8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.31.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok) (2.1.3)\n",
            "Installing collected packages: flask_ngrok\n",
            "Successfully installed flask_ngrok-0.0.25\n",
            "Collecting flask-bootstrap\n",
            "  Downloading Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-bootstrap) (2.2.5)\n",
            "Collecting dominate (from flask-bootstrap)\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visitor (from flask-bootstrap)\n",
            "  Downloading visitor-0.1.3.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-bootstrap) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-bootstrap) (2.1.3)\n",
            "Building wheels for collected packages: flask-bootstrap, visitor\n",
            "  Building wheel for flask-bootstrap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flask-bootstrap: filename=Flask_Bootstrap-3.3.7.1-py3-none-any.whl size=460119 sha256=236a35a2722c9bb8ae240a9235636e76920ceb4e6a5aed0fd2708cfb96be98f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/33/ad/26540e84a28334e5dfeda756df270f95353779f03bc5cf40d4\n",
            "  Building wheel for visitor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visitor: filename=visitor-0.1.3-py3-none-any.whl size=3925 sha256=18eae34b2f20de3120f802a0e41e9f2364df5bf2f98f5ee65654755587f56a0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/31/99/2ec5b4459cac4d801d6201d501a354366d180afc9f8bb2d333\n",
            "Successfully built flask-bootstrap visitor\n",
            "Installing collected packages: visitor, dominate, flask-bootstrap\n",
            "Successfully installed dominate-2.8.0 flask-bootstrap-3.3.7.1 visitor-0.1.3\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok==4.1.1\n",
        "!pip install flask_ngrok\n",
        "!pip install flask-bootstrap\n",
        "!ngrok authtoken '2QIX4LnKgVCm7rgxFme5TXTkJVZ_3scHFqy3ANZ8VHVqxAKPq'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcCyYevxEIlc",
        "outputId": "04ca5be4-b8a6-4671-b67a-56c630033fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "\n",
        "# flask_ngrok_example.py\n",
        "from flask import Flask\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Download das stopwords se ainda não tiver sido feito\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Documentos de exemplo\n",
        "documentos = [\n",
        "    \"O rato roeu a roupa do rato.\",\n",
        "    \"Ferrari paz amor.\",\n",
        "    \"Outro documento de exemplo.\",\n",
        "    \"Algo mais para processar.\"\n",
        "]\n",
        "\n",
        "# Definição de stopwords em português\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Crie um objeto CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Tokenização e segmentação em palavras\n",
        "def word_extraction(text):\n",
        "    words = re.sub(\"[^\\w]\", \" \", text).split()\n",
        "    cleaned_text = [w.lower() for w in words if w.lower() not in stopwords_pt]\n",
        "    return ' '.join(cleaned_text)\n",
        "\n",
        "# Processamento dos documentos e criação da matriz TF\n",
        "documentos_processados = [word_extraction(doc) for doc in documentos]\n",
        "matriz_tf = vectorizer.fit_transform(documentos_processados)\n",
        "matriz_tf_array = matriz_tf.toarray()\n",
        "\n",
        "# Obtenha os nomes das palavras (features) como colunas da matriz\n",
        "nomes_palavras = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Função para processar o texto\n",
        "def pipeline(text):\n",
        "    # Vetorização do texto\n",
        "    vetorizado = vectorizer.transform([text]).toarray()\n",
        "    return {'matriz_tf_vetorizada': vetorizado.tolist()}\n",
        "\n",
        "@app.route('/api/matriz_tf', methods=['POST'])\n",
        "def obter_matriz_tf():\n",
        "    if 'text' in request.json:\n",
        "        texto = request.json['text']\n",
        "        response = pipeline(texto)\n",
        "        return jsonify(response)\n",
        "    else:\n",
        "        return jsonify({'error': 'Texto não encontrado na solicitação.'}), 400\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yxIv5J49XC7H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
